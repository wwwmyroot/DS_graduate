# -*- coding: utf-8 -*-
"""C-01_v03_my_VKR_rev3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tp54u5cRW2Utuw00-PR5AK0csVO37KD1

## _ 01. Импорт.
"""

# Commented out IPython magic to ensure Python compatibility.
# ---- pip install
! pip install xgboost
! pip install scikeras[tensorflow]

# ---- база
import numpy as np                                      # для работы с многомерными массивами 
import pandas as pd                                     # для обработки данных
import matplotlib.pyplot as plt                         # для построения графиков
import seaborn as sns                                   # для статистической визуализации
                
# ---- работа с датафреймами
from pandas import DataFrame

# ---- sklearn
from sklearn.preprocessing import LabelEncoder          # для кодирования переменных
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, PowerTransformer, Normalizer    # для нормализации и стандартизации
from sklearn.model_selection import train_test_split    # для разделения датасета на тренировочную и тестовую выборки
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error # метрики для алгорритмов машиннного обучения
# -- algorithms & models
from sklearn.neighbors import KNeighborsRegressor       # Метод К-ближайших соседей
from sklearn.svm import SVR                             # Метод опорных векторов
from sklearn.linear_model import LinearRegression       # Линейная регрессия
from sklearn.tree import DecisionTreeRegressor          # Дерево решений
from sklearn.ensemble import AdaBoostRegressor          # AdaBoost
from sklearn.ensemble import GradientBoostingRegressor  # Градиентный бустинг
from sklearn.ensemble import RandomForestRegressor      # Случайный лес
from sklearn.linear_model import SGDRegressor           # Стохастический градиентный спуск
from sklearn.linear_model import Lasso                  # Метод регрессии «Lasso»
from sklearn.pipeline import make_pipeline              # для создания пайплайна
from sklearn.model_selection import GridSearchCV        # для подбора параметров модели машинного обучения
from xgboost import XGBRegressor                        # XGBoost

# ---- scipy
from scipy import stats                                 # Статистические функции
from scipy.stats import shapiro                         # Тест Шапиро-Уилка на нормальность           

# ---- keras
import tensorflow.keras as keras                        
from keras.models import Sequential
from keras.layers import Dense, Flatten, Input, Dropout
from tensorflow.keras import utils
from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad
# -- my -- no import for KerasRegressor -- Error
# from scikeras.wrappers import KerasRegressor
# - solution 1 -- yes
#from tensorflow import keras
#try:
#    import scikeras
#except ImportError:
    #!python -m pip install scikeras
#from scikeras.wrappers import KerasClassifier, KerasRegressor
# --
from scikeras.wrappers import KerasClassifier, KerasRegressor
# ---- model operations
from joblib import dump, load                           # для сохранения и загрузки моделей 
from tensorflow.keras.models import load_model          # для загрузки моделей 

import warnings                                         # контроль предупреждения
warnings.filterwarnings("default", category=FutureWarning)
warnings.simplefilter(action='ignore', category=FutureWarning)

# ---- flask
import flask
from flask import Flask, request, render_template

# для корректного отображения графиков в окне браузера
# %matplotlib inline  

# ---- pretify output
from google.colab import widgets
from google.colab import data_table

"""## _ 02. Загрузка и обработка исходных датасетов."""

# ---- ---- Загрузка исходного датасета из github; --- ----

# -- Разово.  Удалить лишнее из GoogleColab, создать папаку, проверить;
! pwd && ls -la && rm -r /content/sample_data 
! mkdir /content/dataset
print(f'\n---- Мусор ("sample_data") удален, целевая папка создана:\n')
# контроль
!ls -la | grep "dataset"
# Загрузка датасета из гитхаба в папку 'dataset'
print(f'\n---- Грузим исходный датасет.\n')
!wget -qc https://github.com/wwwmyroot/DS_graduate/blob/main/dataset/in_x_bp.xlsx -P "/content/dataset"
!wget -qc https://github.com/wwwmyroot/DS_graduate/blob/main/dataset/in_x_nup.xlsx -P "/content/dataset"
! ls -la /content/dataset/
print(f'\n---- Готово.\n')
# -- p_fin --
t = widgets.TabBar(["- план -", " - результат -"])
with t.output_to(0):
  print("1) Удалить папку 'sample_data'.")
  print("2) Cоздать папку 'dataset'.")
  print("3) Загрузить характеристики базальтопластика.")
  print("4) Загрузить характеристики нашивки из углепластика.")
  print("5) Всё проверить.")

with t.output_to(1):
  print("1) Удалена папка 'sample_data'.")
  print("2) Cоздана целевая папка 'dataset'.")
  print("3) Загружены характеристики базальтопластика. -> Файл 'in_x_bp.xlsx'.")
  print("4) Загружены характеристики нашивки из углепластика.-> Файл 'in_x_nup.xlsx'. ")
  print("5) Всё проверено. OK.")

"""**Первый осмотр сырых данных.**"""

# ---- Просмотр данных в файле (свойства базальтопластика); 
dataset_bp = pd.read_excel('/content/dataset/in_x_bp.xlsx')
dataset_bp.head()

dataset_bp.shape

dataset_bp.tail()

# 1
dataset_bp = pd.read_excel('/content/dataset/in_x_bp.xlsx', index_col=0)
dataset_bp.head()

dataset_bp.shape

dataset_nup = pd.read_excel('/content/dataset/in_x_nup.xlsx')
dataset_nup.head()

dataset_nup.shape

# 2
dataset_nup.drop(['Unnamed: 0'], axis=1, inplace=True)
dataset_nup.head()

dataset_nup.shape

# inner join
dataset = pd.merge(dataset_bp, dataset_nup, 
                   left_index=True, 
                   right_index=True, 
                   how = "inner")
dataset.head()

dataset.shape

dataset.to_excel('/content/dataset/in_x_full.xlsx')

df = dataset.copy()

"""## _ 03. Разведочный анализ данных."""

df

df.info()

df.isna().sum()

df.nunique()

df['Угол нашивки, град'].value_counts()

df['Угол нашивки, град'] = df['Угол нашивки, град'].map({0.0:0, 90.0:1}).astype(int)

df = df.rename(columns={'Угол нашивки, град': 'Угол нашивки'})
df

column_names = df.columns
column_names

df.describe()

mean_median = pd.DataFrame({'Среднее': df.mean(axis=0),
                            'Медиана': df.median(axis=0),
                            'Разница м/у средним и медианой': df.mean(axis=0) - df.median(axis=0)})
mean_median

# 1. 
df.corr(method = 'pearson')

"""Вывод: Коэффициент корреляции Пирсона. Cущественная зависимость между пременными отсутствует."""

# 2. 
df.corr(method = 'spearman')

"""Вывод: Корреляция Спирмена. Зависимость между переменными не наблюдается."""

# 3.
df.corr(method = 'kendall')

"""Вывод: Коэффициент корреляции Кендалла. Статистическая зависимость между пременными отсутствует."""

# viz 1.
df.hist(figsize = (15,15), color = 'darkmagenta')
plt.suptitle('Гистограммы распределения', fontsize=20,  y=0.95)
plt.show()

# viz 2. 
n_subplots = len(df.columns)                        
n_cols = 3                                          
n_rows = (n_subplots + n_cols - 1) // n_cols        

plt.figure(figsize=(15,20))
plt.suptitle('Гистограммы и графики плотности', fontsize=20,  y=0.92)
for i, col in enumerate(df.columns, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.histplot(df[col], kde=True, color='darkmagenta') 
    plt.ylabel("")
plt.show()

# viz 3.
n_subplots = len(df.columns)                        
n_cols = 4                                          
n_rows = (n_subplots + n_cols - 1) // n_cols        

plt.figure(figsize=(20,25))
plt.suptitle('Диаграммы "ящик с усами"', fontsize=20,  y=0.92)
for i, col in enumerate(df.columns, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.boxplot(data=df[col], color='mediumorchid') 
    plt.title(df[col].name, size=12)
plt.show()

# viz 4. 
plt.figure(figsize=(20,20))
sns.set(style='white', palette='RdPu_r')
sns.pairplot(df)
plt.show()

# viz 5.
plt.figure(figsize=(20,20))
sns.set(style='white', palette='RdPu_r')
sns.pairplot(df, hue='Угол нашивки')
plt.show()

# viz 6. 
plt.figure(figsize=(20,20))
sns.set(style='white', palette='RdPu_r')

g = sns.PairGrid(df)
g.map_upper(plt.scatter)                   
g.map_diag(sns.histplot, kde=True)         
g.map_lower(sns.kdeplot)                   
plt.show()

# viz 7. 
f, ax = plt.subplots(figsize=(12,10))
sns.heatmap(df.corr(), annot=True, ax=ax, square = True, cmap='RdPu')
plt.show()

"""### _ 04.1. Работа с выбросами.

TODO m1

-----------------------------------------------------------------------------------------------------------------

TODO m2

todo Itogo.

Вывод: выбросы из данных удалены после трех этапов очистки.

### _ 04.2. Нормализация и стандартизация данных.

Проверку на нормальность можно проводить несколькими способами:

* построение гистограммы признака
* построение QQ-графика
* проведение теста на нормальность

#### _ 04.2.1. Проверка на нормальность.

#### _ 04.2.2. Нормализация данных.

##### _04.2.2.1. MinMaxScaler.

##### _ 04.2.2.2. RobustScaler.

#### _ 04.2.3. Стандартизация данных.

##### _ 04.2.3.2. StandardScaler.

##### _ 04.2.3.3. Normalizer.

Все значения теперь находятся в диапазоне от 0 до 1.

##### _ 04.2.3.4. PowerTransformer.
"""

# 1.5. Преобразование Йео-Джонсона из метода PowerTransformer()
yeo = PowerTransformer(method='yeo-johnson', standardize=True)                          
dataset_norm_yeo = yeo.fit_transform(np.array(df_clean[df_clean.columns]))              
df_norm_yeo = pd.DataFrame(data = dataset_norm_yeo, columns = df_clean.columns)         
df_norm_yeo.describe()

plt.figure(figsize=(15,12))
sns.set(context='notebook', style='whitegrid')
sns.kdeplot(data=df_norm_yeo)
plt.show()

"""#### _04.2.4. Проверка на нормальность после нормализации."""

# 1. Тест Шапиро-Уилка на нормальность
for col in df_norm_minmax.columns:
    print(df_norm_minmax[col].name, shapiro(df_norm_minmax[col]))

# 2. Гистограммы расределения после нормализации
df_norm_minmax.hist(figsize = (15,15), color = 'mediumorchid')
plt.suptitle('Гистограммы после нормализации', fontsize=20,  y=0.95)
plt.show()

# 3. QQ-графики после нормализации
n_subplots = len(df_norm_minmax.columns)                  
n_cols = 4                                                
n_rows = (n_subplots + n_cols - 1) // n_cols              

plt.figure(figsize=(20,25))
plt.suptitle('Распределение после нормализации', fontsize=20,  y=0.92)
for i, col in enumerate(df_norm_minmax.columns, 1):
    plt.subplot(n_rows, n_cols, i)
    stats.probplot(df_norm_minmax[col], dist='norm', plot=plt)
    plt.title(df_norm_minmax[col].name, size=12)
plt.show()

"""TODO  выводы

## _ 05.  Модели для прогноза модуля упругости при растяжении и прочности при растяжении.

В случае прогнозирования модуля упругости при растяжении и прочности при растяжении будет решаться задача регрессии. Задача регрессии – это задача обучения с учителем.

### _ 05.1. Прогнозирование модуля упругости.
"""

print('Размер тренировочного датасета: {}\nРазмер тестового датасета:{}'.format(X_train_elastic.shape, X_test_elastic.shape))

X_train_elastic

y_train_elastic

"""TODO  выводы

### _05.2. Прогнозирование прочности при растяжении.

TODO  выводы

## _06. Нейронная сеть для рекомендации соотношения "матрица - наполнитель".

## _07. Разработка приложения для прогнозирования соотношения "матрица-наполнитель"

## _08. Выводы:
"""