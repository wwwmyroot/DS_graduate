# -*- coding: utf-8 -*-
"""v02_my_VKR_rev1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zpm8JWJS9ExsMitfmnS-qjBXK1FC2nOX

## _ 01. Импорт библиотек.
"""

# Commented out IPython magic to ensure Python compatibility.
# ---- pip install
! pip install xgboost
! pip install scikeras[tensorflow]

# ---- база
import numpy as np                                      # для работы с многомерными массивами 
import pandas as pd                                     # для обработки данных
import matplotlib.pyplot as plt                         # для построения графиков
import seaborn as sns                                   # для статистической визуализации
                
# ---- работа с датафреймами
from pandas import DataFrame

# ---- sklearn
from sklearn.preprocessing import LabelEncoder          # для кодирования переменных
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, PowerTransformer, Normalizer    # для нормализации и стандартизации
from sklearn.model_selection import train_test_split    # для разделения датасета на тренировочную и тестовую выборки
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error # метрики для алгорритмов машиннного обучения
# -- algorithms & models
from sklearn.neighbors import KNeighborsRegressor       # Метод К-ближайших соседей
from sklearn.svm import SVR                             # Метод опорных векторов
from sklearn.linear_model import LinearRegression       # Линейная регрессия
from sklearn.tree import DecisionTreeRegressor          # Дерево решений
from sklearn.ensemble import AdaBoostRegressor          # AdaBoost
from sklearn.ensemble import GradientBoostingRegressor  # Градиентный бустинг
from sklearn.ensemble import RandomForestRegressor      # Случайный лес
from sklearn.linear_model import SGDRegressor           # Стохастический градиентный спуск
from sklearn.linear_model import Lasso                  # Метод регрессии «Lasso»
from sklearn.pipeline import make_pipeline              # для создания пайплайна
from sklearn.model_selection import GridSearchCV        # для подбора параметров модели машинного обучения
from xgboost import XGBRegressor                        # XGBoost

# ---- scipy
from scipy import stats                                 # Статистические функции
from scipy.stats import shapiro                         # Тест Шапиро-Уилка на нормальность           

# ---- keras
import tensorflow.keras as keras                        
from keras.models import Sequential
from keras.layers import Dense, Flatten, Input, Dropout
from tensorflow.keras import utils
from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad
# -- my -- no import for KerasRegressor -- Error
# from scikeras.wrappers import KerasRegressor
# - solution 1 -- yes
from tensorflow import keras
try:
    import scikeras
except ImportError:
    !python -m pip install scikeras
from scikeras.wrappers import KerasClassifier, KerasRegressor
# --

# ---- model operations
from joblib import dump, load                           # для сохранения и загрузки моделей 
from tensorflow.keras.models import load_model          # для загрузки моделей 

import warnings                                         # контроль предупреждения
warnings.filterwarnings("default", category=FutureWarning)
warnings.simplefilter(action='ignore', category=FutureWarning)

# ---- flask
import flask
from flask import Flask, request, render_template

# для корректного отображения графиков в окне браузера
# %matplotlib inline  

# ---- pretify output
from google.colab import widgets
from google.colab import data_table

"""## _ 02. Загрузка и обработка исходных датасетов."""

!wget -qc https://github.com/wwwmyroot/DS_graduate/blob/main/dataset/in_x_bp.zip -P "/content/dataset"
!wget -qc https://github.com/wwwmyroot/DS_graduate/blob/main/dataset/in_x_nup.zip -P "/content/dataset"

# ---- ---- Загрузка исходного датасета из github; --- ----

# -- Разово.  Удалить лишнее из GoogleColab, создать папаку, проверить;
# ! pwd && ls -la && rm -r /content/sample_data 
# ! mkdir /content/dataset
print(f'\n---- Мусор ("sample_data") удален, целевая папка создана:\n')
# контроль
!ls -la | grep "dataset"
# Загрузка датасета из гитхаба в папку 'dataset'
# Для предотвращения ошибок с GoogleCollab (set engine manual; ) - загрузим формат *.csv
# периодические ошибки:
# 1) select engine manually; 
# 2) Error tokenizing data. C error: Expected 1 fields in line 28, saw 367
print(f'\n---- Грузим исходный датасет.\n')
# !wget -qc https://github.com/wwwmyroot/DS_graduate/blob/main/dataset/in_x_bp.xlsx -P "/content/dataset"
# !wget -qc https://github.com/wwwmyroot/DS_graduate/blob/main/dataset/in_x_nup.xlsx -P "/content/dataset"
# !wget -qc https://github.com/wwwmyroot/DS_graduate/blob/main/dataset/in_x_bp.csv -P "/content/dataset"
# !wget -qc https://github.com/wwwmyroot/DS_graduate/blob/main/dataset/in_x_nup.csv -P "/content/dataset"
!wget -qc https://github.com/wwwmyroot/DS_graduate/blob/main/dataset/in_1col_x_bp.csv -P "/content/dataset"
!wget -qc https://github.com/wwwmyroot/DS_graduate/blob/main/dataset/in_1col_x_nup.csv -P "/content/dataset"
print(f'\n---- Готово.\n')
! ls -la /content/dataset/
print(f'\n---- Готово.\n')
# -- p_fin --
t = widgets.TabBar(["-| план |-", "-| результат |-"])
with t.output_to(0):
  print("1) Удалить папку 'sample_data'.")
  print("2) Cоздать папку 'dataset'.")
  print("3) Загрузить характеристики базальтопластика.")
  print("4) Загрузить характеристики нашивки из углепластика.")
  print("5) Всё проверить.")

with t.output_to(1):
  print("1) Удалена папка 'sample_data'.")
  print("2) Cоздана целевая папка 'dataset'.")
  print("3) Загружены характеристики базальтопластика. -> Файл 'X_bp.xlsx'.")
  print("4) Загружены характеристики нашивки из углепластика.-> Файл 'X_nup.xlsx'. ")
  print("5) Всё проверено. OK.")

"""**Первый просмотр сырого датасета.**"""

# Загрузим датасет из файла "X_bp.xlsx" (характеристики базальтопластика) и выведем 5 первых позиций датасета
# Для GoogleCollab - *.csv 
# Дурная плавающая ошибка от запуска GoogleCollab к запуску - то ругается
# на движки при чтении .xlsx, то на сам .csv если нет названия первого столбца, то на то, что это не .zip ...
# Устал
# dataset_bp = pd.DataFrame(pd.read_excel("/content/dataset/in_x_bp.xlsx", engine="openpyxl", skiprows=0))
# dataset_nup = pd.DataFrame(pd.read_excel("/content/dataset/in_x_nup.xlsx", engine="openpyxl", skiprows=0))
# dataset_bp = pd.DataFrame(pd.read_excel("/content/dataset/in_x_bp.xlsx", engine="xlrd"))
# dataset_nup = pd.DataFrame(pd.read_excel("/content/dataset/in_x_nup.xlsx", engine="xlrd"))
# dataset_bp = pd.DataFrame(pd.read_csv("/content/dataset/in_1col_x_bp.csv", sep="\t"))
# dataset_nup = pd.DataFrame(pd.read_csv("/content/dataset/in_1col_x_nup.csv", sep="\t"))
# dataset_bp = pd.DataFrame(pd.read_csv("/content/dataset/in_1col_x_bp.csv", sep=","))
# dataset_nup = pd.DataFrame(pd.read_csv("/content/dataset/in_1col_x_nup.csv", sep=","))
# dataset_bp = pd.read_csv("/content/dataset/in_1col_x_bp.csv", delimiter=',', skiprows=0, low_memory=False)
# dataset_nup = pd.read_csv("/content/dataset/in_1col_x_nup.csv", delimiter=',', skiprows=0, low_memory=False)
# dataset_bp = pd.read_csv("/content/dataset/in_1col_x_bp.csv", sep="\t", skiprows=0, low_memory=False)
# dataset_nup = pd.read_csv("/content/dataset/in_1col_x_nup.csv", sep="\t", skiprows=0, low_memory=False)
# dataset_bp = pd.DataFrame(pd.read_excel("/content/dataset/in_x_bp.xlsx", engine="openpyxl"))
# dataset_nup = pd.DataFrame(pd.read_excel("/content/dataset/in_x_nup.xlsx", engine="openpyxl"))
# dataset_bp = pd.DataFrame(pd.read_csv("/content/dataset/in_1col_x_bp.csv"))
# dataset_nup = pd.DataFrame(pd.read_csv("/content/dataset/in_1col_x_nup.csv"))
dataset_bp = pd.read_csv("/content/dataset/in_1col_x_bp.csv")
dataset_nup = pd.read_csv("/content/dataset/in_1col_x_nup.csv")
# dataset_bp.enable_dataframe_formatter()
dataset_bp.head()

# Посмотрим размерность первого датасета dataset_bp 
dataset_bp.shape

# Выведем последних 5 позиций датасета
dataset_bp.tail()

"""Столбец "Unnamed: 0" является неинформативным, т.к. содержит порядковые номера композитов в таблице и совпадает со столбцом индекса.
Удалить данный столбец можно 2 способами: 

1 - непосредственно при загрузке файла, заменив столбец с индексами столбцом "Unnamed: 0".

2 - используя функцию drop.
"""

# Воспользуемся первым способом для удаления столбца "Unnamed: 0" и перезагрузим датасет. Выведем 5 первых строк.
dataset_bp = pd.read_excel('/content/X_bp.xlsx', index_col=0)
dataset_bp.head()

# Посмотрим размерность первого датасета dataset_bp после замены столбца
dataset_bp.shape

# Загрузим датасет из файла "X_nup.xlsx" (характеристики нашивки из углепластика) и выведем 5 первых позиций датасета
dataset_nup = pd.read_excel('/content/X_nup.xlsx')
dataset_nup.head()

# Посмотрим размерность второго датасета dataset_nup
dataset_nup.shape

# Воспользуемся вторым способом (с помощью функции drop) для удаления столбца "Unnamed: 0". Выведем 5 первых строк.
dataset_nup.drop(['Unnamed: 0'], axis=1, inplace=True)
dataset_nup.head()

# Посмотрим размерность второго датасета dataset_nup после удаления столбца
dataset_nup.shape

"""Датасеты имеют разное количество строк: 1023 строки у dataset_bp, 1040 строк у dataset_nup"""

# Объединеним датесеты в один
# Объединение выполним с помощью функции merge() по индексу с типом объединения INNER
dataset = pd.merge(dataset_bp, dataset_nup, 
                   left_index=True, 
                   right_index=True, 
                   how = "inner")

# Выведем 5 первых позиций нового датасета
dataset.head()

# Посмотрим размерность объединенного датасета
dataset.shape

"""Объединенный датасет содержит 1023 строки и 13 столбцов. Это означает, что часть данных (а именно 17 строк из датасета dataset_nup) была удалена из таблицы и исключена из дальнейшего исследования."""

# Сохраним объединенный датасет в новый файл для дальнейшей работы
dataset.to_excel('/content/X_full.xlsx')

# Создадим копию объединенного датасета для дальнейшей работы
df = dataset.copy()

"""## _ 03. Разведочный анализ данных."""

# Датасет для дальнейшей работы выглядит следующим образом:
df

# Посмотрим информацию обо всех столбцах датасета
df.info()

"""Датасет состоит из 13 столбцов.

Пропусков в датасете нет, каждый столбец содержит 1023 значения.

Тип данных большинства столбцов - float64 (числа с плавающей точкой), только столбец "Угол нашивки, град" имеет тип int64 (целые числа). 
"""

# Проверить отсутствие пропусков можно также используя функцию isna()
df.isna().sum()

"""Т.к. пропуски в данных отсутствуют, очистка датасета не потребуется."""

# Посмотрим количество уникальных значений по всем столбцам датасета с помощью функции nunique()
df.nunique()

"""Большинство столбцов содержит около тысячи уникальных значений, кроме столбца "Угол нашивки, град", где уникальных значений всего 2."""

# Посмотрим значения и их количество для столбца "Угол нашивки, град"
df['Угол нашивки, град'].value_counts()

"""В столбце "Угол нашивки, град" 520 раз встречается значение "0" и 503 раза - значение "90". Т.к. другие значения данного признака в датасете не встречаются, будем считать его бинарным и категориальным."""

# Для дальнейшей работы заменим значения "0" и "90" в столбце "Угол нашивки, град" на 0 и 1 соответственно. 
# Тип данных столбца зададим integer.
df['Угол нашивки, град'] = df['Угол нашивки, град'].map({0.0:0, 90.0:1}).astype(int)

# Уберем из названия столбца "Угол нашивки, град" слово "град" и выведем полученный результат
df = df.rename(columns={'Угол нашивки, град': 'Угол нашивки'})
df

# Посмотрим названия всех столбцов датасета
column_names = df.columns
column_names

# Посмотрим числовые статистики с помощью метода describe() 
df.describe()

"""Здесь:
   * count - количество элементов;
   * mean - среднее арифметическое;
   * std - стандартное отклонение;
   * min, max - минимальное и максимальное значения соответсвенно;
   * 25%, 50%, 75% - 25, 50 и 75 процентные перцентили.
"""

# Выведем отдельно средние и медианы по столбцам и проанализируем эти значения
mean_median = pd.DataFrame({'Среднее': df.mean(axis=0),
                            'Медиана': df.median(axis=0),
                            'Разница м/у средним и медианой': df.mean(axis=0) - df.median(axis=0)})
mean_median

"""Статистики, полученные при помощи метода describe(), совпадают со значениями статистик, рассчитанных с применением функций mean() и median().

Среднее значение не является надежным инструментом, поскольку на него сильно влияют выбросы.	
Медиана лучше подходит для искаженных распределений, чтобы получить центральную тенденцию, поскольку она намного более устойчива и разумна.

Из графиков следует, что средние значения анализируемых параметров не значительно отличаются от значений их медиан, что позволяет предположить небольшое количество выбросов в данных.
"""

# Проведем анализ зависимостей между переменнными
# Для этого рассчитаем значения коэффициентов корреляции
# 1. Коэффициент корреляции Пирсона
df.corr(method = 'pearson')

"""Вывод: существенная зависимость между пременными отсутствует."""

# 2. Коэффициент корреляции Спирмена
df.corr(method = 'spearman')

"""Вывод: зависимость между переменными не наблюдается."""

# 3. Коэффициент корреляции Кендалла
df.corr(method = 'kendall')

"""Вывод: статистическая зависимость между пременными отсутствует."""

# Визуализируем исходный (необработанный) датасет
# 1. Гистограммы расределения
df.hist(figsize = (15,15), color = 'darkmagenta')
plt.suptitle('Гистограммы распределения', fontsize=20,  y=0.95)
plt.show()

"""Гистограмма — это классический инструмент визуализации, который представляет собой распределение одной или нескольких переменных путем подсчета количества наблюдений, попадающих в интервалы разбиения. 

Анализ графиков показывает, что распределения всех параметров (кроме "Угла нашивки" и "Поверхностной плотности, г/м2") стремятся к нормальному. Параметр "Угол нашивки" имеет два значения: 0 и 1, что соответствует величинам 0 и 90 градусов. В дальнейшем целесообразно рассматривать данный признак как категориальный.

Связанный с гистограммой тип графиков — график плотности, который можно назвать непрерывным сглаженным аналогом гистограммы. Самый распространённый вариант построения такого графика — ядерная оценка плотности. В этом методе для каждой точки данных строится непрерывная кривая — ядро. Все эти кривые складываются вместе, чтобы получить единую гладкую оценку плотности.
"""

# 2. Гистограммы и графики плотности
n_subplots = len(df.columns)                        # считаем количество графиков
n_cols = 3                                          # задаем количество столбцов
n_rows = (n_subplots + n_cols - 1) // n_cols        # вычисляем количество строк

plt.figure(figsize=(15,20))
plt.suptitle('Гистограммы и графики плотности', fontsize=20,  y=0.92)
for i, col in enumerate(df.columns, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.histplot(df[col], kde=True, color='darkmagenta') 
    plt.ylabel("")
plt.show()

"""Графики, приведенные выше, свидетельствуют о том, что распределения всех параметров (кроме "Угла нашивки" и "Поверхностной плотности, г/м2") стремятся к нормальному. Параметр "Угол нашивки" имеет два значения: 0 и 1."""

# 3. Диаграммы "ящик с усами"
n_subplots = len(df.columns)                        # считаем количество графиков
n_cols = 4                                          # задаем количество столбцов
n_rows = (n_subplots + n_cols - 1) // n_cols        # вычисляем количество строк

plt.figure(figsize=(20,25))
plt.suptitle('Диаграммы "ящик с усами"', fontsize=20,  y=0.92)
for i, col in enumerate(df.columns, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.boxplot(data=df[col], color='mediumorchid') 
    plt.title(df[col].name, size=12)
plt.show()

"""Boxplot - график, использующийся в описательной статистике, компактно изображающий одномерное распределение вероятностей.

Такой вид диаграммы в удобной форме показывает медиану, нижний и верхний квантили, минимальное и максимальное значение выборки и выбросы. 


Выброс (в статистике) — это измерительная точка данных, которая значительно выделяется из общей выборки. Выбросы могут быть вызваны вариативностью измерений или указывать на экспериментальную ошибку; в последнем случае они иногда исключаются из набора данных. Выброс может вызвать серьезные проблемы при статистическом анализе.

В boxplot можно считать нижний и верхний усики как о границы распределения данных. Любые точки данных, которые показывают выше или ниже усов, могут считаться выбросами или аномальными.

Из диаграмм "ящик с усами" видно, что выбросы имеют все характеристики, кроме параметра "Угол нашивки".

Влияние выбросов на модель:

* Данные оказались в искаженном формате.
* Изменяет общее статистическое распределение данных с точки зрения среднего значения, дисперсии и т.д.
* Приводит к искажению уровня точности модели.

Из-за вышеуказанных причин необходимо обнаружить и избавиться от выбросов до моделирования набора данных.
"""

# 4. Попарные графики рассеяния точек с помощью функции sns.pairplot()
plt.figure(figsize=(20,20))
sns.set(style='white', palette='RdPu_r')
sns.pairplot(df)
plt.show()

# Попарные графики рассеяния точек с помощью функции sns.pairplot() 
# с учетом кростабуляции для категориального признака "Угол нашивки"
plt.figure(figsize=(20,20))
sns.set(style='white', palette='RdPu_r')
sns.pairplot(df, hue='Угол нашивки')
plt.show()

"""Матричная диаграмма рассеяния представляет собой все возможные попарные диаграммы рассеяния, представленные в виде большой квадратной матрицы. Диагональные элементы матрицы являются графиками ядерной оценки плотности распределения вероятности каждой из переменных. А остальные элементы — это диаграммы рассеяния переменных относительно друг друга.

Приведенные выше графики свидетельствуют об отсутствии линейной зависимости между характеристиками композитных материалов.

Функция sns.pairplot() является надстройкой (упрощенной версией) другой функции этой библиотеки, sns.PairGrid().
"""

# 5. Попарные графики рассеяния точек с помощью функции sns.pairgrid()
plt.figure(figsize=(20,20))
sns.set(style='white', palette='RdPu_r')

g = sns.PairGrid(df)
g.map_upper(plt.scatter)                   # над диагональю изобразим сами точки
g.map_diag(sns.histplot, kde=True)         # на диагонали расположим одномерные ядерные оценки плотности с гистограммами
g.map_lower(sns.kdeplot)                   # под диагональю - двумерные 
plt.show()

"""Вывод: зависимость между параметрами не наблюдается."""

# 6. Визуализируем матрицу корреляции в виде тепловой карты
f, ax = plt.subplots(figsize=(12,10))
sns.heatmap(df.corr(), annot=True, ax=ax, square = True, cmap='RdPu')
plt.show()

"""Вывод: зависимость между всеми параметрами крайне слабая. 
Наибольшая корреляция наблюдается между "Плотностью нашивки" и "Углом нашивки" и равняется 0,11.

## _ 04.  Предобработка данных.

### _ 04.1. Работа с выбросами.

Произведем поиск и удаление выбросов в данных. 

Для поиска выбросов воспользуемся двумя методами:
* 1) Стандартное отклонение (правило трех сигм)
* 2) Метод межквартильного диапазона (IQR - Inter Quartile Range)

Метод 1 - Стандартное отклонение (правило трех сигм): когда данные подчиняются нормальному распределению, 99,7% значений должны находиться в пределах 3 стандартных отклонений от среднего; когда значение превышает это расстояние, это можно считать выбросом.

Для реализации этого метода применяют z-оценку. Z-оценка показывает количество стандартных отклонений данного значения от среднего. Формула для расчета z-показателя:

z = (X - μ) / σ

где:
* X — это одно необработанное значение данных.
* μ - среднее значение 
* σ - стандартное отклонение 

Наблюдение можно идентифицировать как выброс, если его z-оценка меньше -3 или больше 3.
Выбросы = наблюдения с z-показателями > 3 или < -3.

-----------------------------------------------------------------------------------------------------------------

Метод 2 - Метод межквартильного диапазона (IQR - Inter Quartile Range): 

Межквартильный размах (IQR) — это разница между 75-м процентилем и 25-м процентилем в наборе данных. Он измеряет разброс средних 50% значений.

Другими словами, IQR - это первый квартиль (Q1), вычтенный из третьего квартиля (Q3); эти квартили можно четко увидеть на диаграмме "ящик с усами".

IQR = Q3 - Q1

Наблюдение можно рассматривать как выброс, если оно в 1,5 раза превышает межквартильный размах, превышающий третий квартиль (Q3), или в 1,5 раза превышает межквартильный размах, меньше первого квартиля (Q1).

Выбросы = наблюдения > Q3 + 1,5*IQR или Q1 – 1,5*IQR
"""

# 1. Поиск анамалий в признаках
# Метод 1 - Стандартное отклонение (правило трех сигм)
# Для этого для каждого значения всех параметров необходимо получить его z-оценку

# Создадим функцию для поиска выбросов
def detect_outliers_zscore(data):
    outliers = []                          # создаем пустой список, куда будем записывать выбросы
    mean = np.mean(data)                   
    std = np.std(data)                     
    for i in data:
        z_score = (i-mean)/std
        if (np.abs(z_score) > 3):
            outliers.append(i)
    return outliers

sum_of_outliers = 0                        # счетчик количества выбросов
for col in df.columns:
    df_outliers_zscore = detect_outliers_zscore(df[col])
    sum_of_outliers += len(df_outliers_zscore)
    print("Количество выбросов в столбце ", df[col].name, ": ", len(df_outliers_zscore))
print("Всего выбросов при использовании метода трех сигм: ", sum_of_outliers)

# Метод 2 - Метод межквартильного диапазона (IQR - Inter Quartile Range):
def detect_outliers_IQR(data):
    outliers = []                          # создаем пустой список, куда будем записывать выбросы
    Q1 = data.quantile(0.25)                
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR           # нижняя граница
    upper_bound = Q3 + 1.5 * IQR           # верхняя граница
    for i in data:
        if (i <= lower_bound) | (i >= upper_bound):
            outliers.append(i)
    return outliers

sum_of_outliers = 0                        # счетчик количества выбросов
for col in df.columns:
    df_outliers_IQR = detect_outliers_IQR(df[col])
    sum_of_outliers += len(df_outliers_IQR)
    print("Количество выбросов в столбце ", df[col].name, ": ", len(df_outliers_IQR))
print("Всего выбросов при использовании метода межквартильного диапазона: ", sum_of_outliers)

"""При использовании метода трех сигм было выявлено 25 выбросов, при использовании метода межквартильного диапазона - 93 выброса.

Подход к поиску выбросов в интервале между квартилями является наиболее часто используемым и наиболее надежным подходом, применяемым в области исследований.

Для дальнейшей работы с выбросами воспользуемся результатами второго метода.
"""

# 2. Работа с выбросами
# 2.1. Удалим выбросы, используя метод межквартильного диапазона

# Созадим копию дататсета для дальнейшей работы
df_clean = df.copy()

# Создадим пустой список для индексов пропусков, которые необходимо удалить
idx_outlier_list = []

for col in df_clean.columns:
    # Находим выбросы
    df_outliers_IQR = detect_outliers_IQR(df_clean[col])
    # Находим индексы элементов, которые отнесли к выбросам
    idx_outlier = np.where(df_clean[col].isin(df_outliers_IQR))[0] 
    # Заполняем список индексов
    idx_outlier_list.extend(idx_outlier)

# Удаляем выбросы из датасета исходя из их индексов    
df_clean = df_clean.drop(idx_outlier_list) 

# Обновим индексы датасета
df_clean.reset_index(drop=True, inplace=True)

# Посмотрим размерность очищенного датасета
df_clean.shape

# Проверим остались ли в очищенном датасете выбросы
sum_of_outliers_2 = 0                       
for col in df_clean.columns:
    df_outliers_IQR = detect_outliers_IQR(df_clean[col])
    sum_of_outliers_2 += len(df_outliers_IQR)
    print("Количество выбросов в столбце ", df_clean[col].name, ": ", len(df_outliers_IQR))
print("Всего выбросов при использовании метода межквартильного диапазона: ", sum_of_outliers_2)

# 2.2. Повторно удалим выбросы, т.к. в данных присутствуют 10 выбросов

# Создадим пустой список для индексов пропусков, которые необходимо удалить
idx_outlier_list_2 = []

for col in df_clean.columns:
    # Находим выбросы
    df_outliers_IQR = detect_outliers_IQR(df_clean[col])
    # Находим индексы элементов, которые отнесли к выбросам
    idx_outlier = np.where(df_clean[col].isin(df_outliers_IQR))[0] 
    # Заполняем список индексов
    idx_outlier_list_2.extend(idx_outlier)

# Удаляем выбросы из датасета исходя из их индексов    
df_clean = df_clean.drop(idx_outlier_list_2) 

# Обновим индексы датасета
df_clean.reset_index(drop=True, inplace=True)

# Посмотрим размерность очищенного датасета
df_clean.shape

# Проверим остались ли в очищенном датасете выбросы
sum_of_outliers_3 = 0                        
for col in df_clean.columns:
    df_outliers_IQR = detect_outliers_IQR(df_clean[col])
    sum_of_outliers_3 += len(df_outliers_IQR)
    print("Количество выбросов в столбце ", df_clean[col].name, ": ", len(df_outliers_IQR))
print("Всего выбросов при использовании метода межквартильного диапазона: ", sum_of_outliers_3)

# 2.3. Еще раз удалим выбросы,  т.к. в данных все еще присутствует 4 выброса

# Создадим пустой список для индексов пропусков, которые необходимо удалить
idx_outlier_list_3 = []

for col in df_clean.columns:
    # Находим выбросы
    df_outliers_IQR = detect_outliers_IQR(df_clean[col])
    # Находим индексы элементов, которые отнесли к выбросам
    idx_outlier = np.where(df_clean[col].isin(df_outliers_IQR))[0] 
    # Заполняем список индексов
    idx_outlier_list_3.extend(idx_outlier)

# Удаляем выбросы из датасета исходя из их индексов    
df_clean = df_clean.drop(idx_outlier_list_3) 

# Обновим индексы датасета
df_clean.reset_index(drop=True, inplace=True)

# Посмотрим размерность очищенного датасета
df_clean.shape

sum_of_outliers_4 = 0                        
for col in df_clean.columns:
    df_outliers_IQR = detect_outliers_IQR(df_clean[col])
    sum_of_outliers_4 += len(df_outliers_IQR)
    print("Количество выбросов в столбце ", df_clean[col].name, ": ", len(df_outliers_IQR))
print("Всего выбросов при использовании метода межквартильного диапазона: ", sum_of_outliers_4)

# Визуализируем очищенный датасет с помощью диаграмм "ящик с усами"
n_subplots = len(df_clean.columns)                        # считаем количество графиков
n_cols = 4                                                # задаем количество столбцов
n_rows = (n_subplots + n_cols - 1) // n_cols              # вычисляем количество строк

plt.figure(figsize=(20,25))
plt.suptitle('Диаграммы "ящик с усами"', fontsize=20,  y=0.92)
for i, col in enumerate(df_clean.columns, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.boxplot(data=df_clean[col], color='mediumorchid') 
    plt.title(df_clean[col].name, size=12)
plt.show()

"""Вывод: выбросы из данных удалены после трех этапов очистки.

### _ 04.2. Нормализация и стандартизация данных.

Многие алгоритмы машинного обучения построены на предположении о Гауссовом (нормальном) распределении входных данных. Поэтому для качественной работы таких моделей, обязательна проверка данных на нормальность, а при необходимости - приведение их к распределению, близкому к нормальному.

Кроме того, в данном датасете присутствуют численные признаки разных масштабов, поэтому необходимо отмасштабировать признаки.

Нормализация - процедура предобработки входной информации (обучающих, тестовых и валидационных выборок, а также реальных данных), при которой значения признаков во входном векторе приводятся к некоторому заданному диапазону, например, [0...1] или [-1...1].

Ключевая цель нормализации - приведение данных в самых разных единицах измерения и диапазонных значениях к единому виду, который позволит сравнить данные между собой и использовать для расчета схожести объектов в выборке. До начала обучения модели необходимо привести все признаки к равному влиянию друг на друга. В Python-библиотеке Scikit-learn есть для этого классы MinMaxScaler и RobustScaler.

Проверку на нормальность можно проводить несколькими способами:

* построение гистограммы признака
* построение QQ-графика
* проведение теста на нормальность

Стандартизация - приведения данных к определенному формату и представлению, которые обеспечивают их корректное применение в многомерном анализе. Стандартизация позволяет устранить возможное влияние отклонений по каждому признаку и приводит все исходные значения в датасете к набору значений к нормальному распределению с математическим ожиданием равным 0 и стандартным отклонением равным 1. В результате получается стандартизированная шкала, которая определяет место каждого значения в наборе данных и измеряет его отклонение от среднего в единицах стандартного отклонения.

Недостатком стандартизации является возможность присутствия в этих шкалах отрицательных значений, что может привести к потере логики анализа данных. Отрицательные значения могут исключаться путем дополнительных преобразований.

Для стандартизации данных в Scikit-learn есть класс StandardScaler, который применяет к каждому из атрибутов следующее: вычитает из значений среднее и делит полученную разность на стандартное отклонение.

Проанализируем данные, полученные после удаления выбросов.
"""

# Посмотрим статистические характеристики датасета после удаления выбросов
df_clean.describe()

# Построим графики распределения всех параметров
plt.figure(figsize=(15,8))
sns.set(context='notebook', style='whitegrid')
sns.kdeplot(data=df_clean)
plt.show()

"""Графики выше показывает, что численные признаки датасета разных масштабов."""

# Выведем отдельно средние и медианы по столбцам и проанализируем эти значения
mean_median = pd.DataFrame({'Среднее': df_clean.mean(axis=0),
                            'Медиана': df_clean.median(axis=0),
                            'Разница м/у средним и медианой': df_clean.mean(axis=0) - df_clean.median(axis=0),
                            'Разница м/у средним и медианой, %': ((df_clean.mean(axis=0) - df_clean.median(axis=0)) / df_clean.median(axis=0)) * 100})
mean_median

"""Медиана и среднее в очищенной выборке откличатся не существенно, кроме столбцов "Поверхностная плотность, г/м2" и "Угол нашивки".

#### _ 04.2.1. Проверка на нормальность.

1. Тест Шапиро-Уилка на нормальность

Тест Шапиро-Уилка используется для определения того, соответствует ли выборка нормальному распределению .

Функция scipy.stats.shapiro() возвращает тестовую статистику и соответствующее p-значение.

Если p-значение ниже определенного уровня значимости (0,05), то у нас есть достаточно доказательств, чтобы сказать, что данные выборки не получены из нормального распределения.
"""

# Проведем тест Шапиро-Уилка на нормальность
for col in df_clean.columns:
    print(df_clean[col].name, shapiro(df_clean[col]))

"""В данном случае нулевая гипотеза отвергается (p-value < 0.05) в случаях:
 - модуль упругости, ГПа
 - Поверхностная плотность, г/м2
 - Модуль упругости при растяжении, ГПа
 - Потребление смолы, г/м2 
 - Угол нашивки
 
что подразумевает, что распределения НЕ является нормальными. 

Однако, проверка нормальности статистическими тестами является очень строгой, т.к. идет сравнение с идеальным распределением. Поэтому несмотря на то, что статистический тест говорит о ненормальности распределения, стоит посмотреть на гистограмму распределения.

2. Гистограммы признаков до стандартизации/нормализации
"""

# Построим гистограммы расределения до нормализации
df_clean.hist(figsize = (15,15), color = 'darkmagenta')
plt.suptitle('Гистограммы до нормализации', fontsize=20,  y=0.95)
plt.show()

"""Анализ графиков показывает, что распределения всех параметров (кроме "Угла нашивки" и "Поверхностной плотности, г/м2") стремятся к нормальному. Параметр "Угол нашивки" имеет два значения: 0 и 1, что соответствует величинам 0 и 90 градусов.

3. QQ-графики

График QQ, сокращение от графика «квантиль-квантиль», используется для оценки того, потенциально ли набор данных получен из некоторого теоретического распределения.

В большинстве случаев этот тип графика используется для определения того, соответствует ли набор данных нормальному распределению.

Если данные распределены нормально, точки на графике QQ будут лежать на прямой диагональной линии.

И наоборот, чем больше точки на графике значительно отклоняются от прямой диагональной линии, тем менее вероятно, что набор данных следует нормальному распределению.
"""

# Построим QQ-графики до нормализации
n_subplots = len(df_clean.columns)                        # считаем количество графиков
n_cols = 4                                                # задаем количество столбцов
n_rows = (n_subplots + n_cols - 1) // n_cols              # вычисляем количество строк

plt.figure(figsize=(20,25))
plt.suptitle('Распределение до нормализации', fontsize=20,  y=0.92)
for i, col in enumerate(df_clean.columns, 1):
    plt.subplot(n_rows, n_cols, i)
    stats.probplot(df_clean[col], dist='norm', plot=plt)
    plt.title(df_clean[col].name, size=12)
plt.show()

"""Из графиков видно, что большинство данных близки к нормальному распределению, которое на данном графике представляет красная линия.

#### _ 04.2.2. Нормализация данных.

##### _04.2.2.1. MinMaxScaler.

Из каждого значения признака, MinMaxScaler вычитает минимальное значение признака и делит на диапазон. Диапазон это разница между минимумом и максимумом этого признака.

MinMaxScaler сохраняет форму исходного распределения. Он не меняет содержательно информацию содержащуюся в исходных данных.

ледует обратить внимание, что MinMaxScaler не снижает важность выбросов.

Диапазон по умолчанию для признака после MinMaxScaler находится между 0 и 1.
"""

# 1.1. MinMaxScaler()
# Выполним масштабирование признаков, чтобы все значения находились в определенном интервале.
# Для масштабирования воспользуемся функцией MinMaxScaler(), по умолчанию - (0,1).
minmax_scl = MinMaxScaler()                                                             # создаем minmax_scl
dataset_norm_minmax = minmax_scl.fit_transform(np.array(df_clean[df_clean.columns]))    # обучаем minmax_scl
df_norm_minmax = pd.DataFrame(data = dataset_norm_minmax, columns = df_clean.columns)   # преобразуем обратно в dataframe
df_norm_minmax.describe()                                                               # выводим описательную статистику

# Визуализируем полученный результат
plt.figure(figsize=(15,12))
sns.set(context='notebook', style='whitegrid')
sns.kdeplot(data=df_norm_minmax)
plt.show()

"""Все параметры имеют одинаковый относительный масштаб. Относительные пробелы между значениями каждого объекта были сохранены.
Максимальное значение признака равно 1, минимальное - 0.
"""

# Для создания приложения для прогнозирования характеристик композитных материалов создадим масштабаторы
minmax_scl_x = MinMaxScaler()
minmax_scl_y = MinMaxScaler()  

# Зададим x и y для нейронной сети
X = df_clean.drop(['Соотношение матрица-наполнитель'], axis = 1)
y = df_clean[['Соотношение матрица-наполнитель']]

# Обучим масштабаторы для x и y
minmax_scl_x.fit(X.values)
minmax_scl_y.fit(y.values)

# Сохраним масштабаторы 
dump(minmax_scl_x, 'minmax_scl_x.pkl')
dump(minmax_scl_y, 'minmax_scl_y.pkl')

a = df_clean.drop(['Соотношение матрица-наполнитель'], axis = 1)
a.shape

"""##### _ 04.2.2.2. RobustScaler.
RobustScaler преобразует вектор признака, вычитая медиану и деля на межквартильный размах (75% значение — 25% значение).
RobustScaler следует использовать для уменьшения эффекта от выбросов по сравнению с MinMaxScaler.

Заметим, что диапазон для каждого признака после применения RobustScaler больше, чем он был для MinMaxScaler.
"""

# 1.2. RobustScaler()
# Cтандартизация признака, однако преобразование происходит с помощью перцентилей, тем самым снижается эффект выбросов
robust_scl = RobustScaler()                                                               # создаем robust_scl
dataset_norm_robust = robust_scl.fit_transform(np.array(df_clean[df_clean.columns]))      # обучаем robust_scl
df_norm_robust = pd.DataFrame(data = dataset_norm_robust, columns = df_clean.columns)     # преобразуем обратно в dataframe
df_norm_robust.describe()                                                                 # выводим описательную статистику

# Визуализируем полученный результат
plt.figure(figsize=(15,12))
sns.set(context='notebook', style='whitegrid')
sns.kdeplot(data=df_norm_robust)
plt.show()

"""После использования RobustScaler() данные приняли отрицательные значения. 
Средние значения находятся в районе 0.
Максимальное значение параметров находится в районе 2, минимальное - в районе -2.

#### _ 04.2.3. Стандартизация данных.

##### _ 04.2.3.2. StandardScaler.
StandardScaler стандартизирует функцию путем вычитания среднего значения, а затем масштабирования до дисперсии единиц. Единица отклонения означает деление всех значений на стандартное отклонение.

StandardScaler приводит к распределению со стандартным отклонением равным 1 и среднем значением распределения  равным 0.
"""

# 1.3. StandardScaler()
# Выполним стандартизацию признаков (вычитание среднего и масштабирование дисперсией)
# Воспользуемся функцией StandardScaler()
std_scl = StandardScaler()                                                              # создаем std_scl
dataset_norm_std = std_scl.fit_transform(np.array(df_clean[df_clean.columns]))          # обучаем std_scl
df_norm_std = pd.DataFrame(data = dataset_norm_std, columns = df_clean.columns)         # преобразуем обратно в dataframe
df_norm_std.describe()                                                                  # выводим описательную статистику

# Визуализируем полученный результат
plt.figure(figsize=(15,12))
sns.set(context='notebook', style='whitegrid')
sns.kdeplot(data=df_norm_std)
plt.show()

"""На графике выше видно, что у всех распределений среднее равно нулю и единичная дисперсия. 
Максимальное значение параметров находится в районе 3, минимальное -  в районе -3.

Алгоритмы глубокого обучения часто требуют нулевого среднего и единичной дисперсии. Регрессионные алгоритмы также выигрывают от нормально распределенных данных с небольшими размерами выборки.

##### _ 04.2.3.3. Normalizer.

Normalizer работает на строках, а не на столбцах. 

По умолчанию, к каждому наблюдению применяется L2 нормализация, так чтобы значения в строке имели единичную норму. Единичная норма в L2 значит, что если мы возведем в квадрат каждый элемент и просуммируем их, в результате получим 1. В качестве альтернативы L2 можно использовать, L1 (манхеттенскую) нормализацию.
"""

# 1.4. Преобразование с помощью Normalizer()
normalize = Normalizer()                                                                            # создаем normalize
dataset_norm_normalizer = normalize.fit_transform(np.array(df_norm_minmax[df_norm_minmax.columns])) # обучаем normalize
df_norm_normalizer = pd.DataFrame(data = dataset_norm_normalizer, columns = df_norm_minmax.columns) # преобразуем обратно в dataframe
df_norm_normalizer.describe()                                                                       # выводим описательную статистику

# Визуализируем полученный результат
plt.figure(figsize=(15,12))
sns.set(context='notebook', style='whitegrid')
sns.kdeplot(data=df_norm_normalizer)
plt.show()

"""Все значения теперь находятся в диапазоне от 0 до 1.

##### _ 04.2.3.4. PowerTransformer.

Преобразование удаляет сдвиг из распределения данных, чтобы сделать распределение более нормальным (гауссовским).

Популярными примерами являются лог-преобразование (положительные значения) или обобщенные версии, такие как преобразование Бокса-Кокса (положительные значения) или преобразование Йео-Джонсона (положительные и отрицательные значения).
"""

# 1.5. Преобразование Йео-Джонсона из метода PowerTransformer()
yeo = PowerTransformer(method='yeo-johnson', standardize=True)                          # создаем yeo
dataset_norm_yeo = yeo.fit_transform(np.array(df_clean[df_clean.columns]))              # обучаем yeo
df_norm_yeo = pd.DataFrame(data = dataset_norm_yeo, columns = df_clean.columns)         # преобразуем обратно в dataframe
df_norm_yeo.describe()                                                                  # выводим описательную статистику

"""При нормализации данных в методе PowerTransformer() был задан параметр стандартизации (standartize=True), который привел распределение к стандартному (вычитание среднего и масштабирование дисперсией)."""

# Визуализируем полученный результат
plt.figure(figsize=(15,12))
sns.set(context='notebook', style='whitegrid')
sns.kdeplot(data=df_norm_yeo)
plt.show()

"""Средние значения находятся в районе 0. Максимальное значение параметров находится в районе 3, минимальное - в районе -3.

Для дальнейшей работы воспользуемся датастом, отформатированным с помощью функции MinMaxScaler().

#### _04.2.4. Проверка на нормальность после нормализации.
"""

# 1. Тест Шапиро-Уилка на нормальность
for col in df_norm_minmax.columns:
    print(df_norm_minmax[col].name, shapiro(df_norm_minmax[col]))

"""После стандартизации нулевая гипотеза отвергается (p-value < 0.05) в случаях:

 - модуль упругости, ГПа
 - Поверхностная плотность, г/м2
 - Модуль упругости при растяжении, ГПа
 - Потребление смолы, г/м2
 - Угол нашивки

что подразумевает, что распределения НЕ являются нормальными.
"""

# 2. Гистограммы расределения после нормализации
df_norm_minmax.hist(figsize = (15,15), color = 'mediumorchid')
plt.suptitle('Гистограммы после нормализации', fontsize=20,  y=0.95)
plt.show()

"""После нормализации изменился диапазон данных: параметры распределены в интервале от -3 до 3. 
Среднее значение равно нулю, дисперсия - 1.
Скорректировались формы графиков распределения, они стали больше похожи на нормальное.
"""

# 3. QQ-графики после нормализации
n_subplots = len(df_norm_minmax.columns)                  # считаем количество графиков
n_cols = 4                                                # задаем количество столбцов
n_rows = (n_subplots + n_cols - 1) // n_cols              # вычисляем количество строк

plt.figure(figsize=(20,25))
plt.suptitle('Распределение после нормализации', fontsize=20,  y=0.92)
for i, col in enumerate(df_norm_minmax.columns, 1):
    plt.subplot(n_rows, n_cols, i)
    stats.probplot(df_norm_minmax[col], dist='norm', plot=plt)
    plt.title(df_norm_minmax[col].name, size=12)
plt.show()

"""После нормализации распределение данных значительно не изменилось.

## _ 05.  Модели для прогноза модуля упругости при растяжении и прочности при растяжении.

В случае прогнозирования модуля упругости при растяжении и прочности при растяжении будет решаться задача регрессии. Задача регрессии – это задача обучения с учителем.
"""

# Создадим копию нормализованного датасета для дальнейшей работы
df_norm = df_norm_minmax.copy()

"""### _ 05.1. Прогнозирование модуля упругости."""

# Разделим датасет на тренировочную и тестовую выборки. 
# При построении модели 30% данных оставим на тестирование модели, на остальных происходит обучение моделей. 
X_train_elastic, X_test_elastic, y_train_elastic, y_test_elastic = train_test_split(
                                                                   df_norm.drop(['Модуль упругости при растяжении, ГПа', 'Прочность при растяжении, МПа'], axis=1), 
                                                                   df_norm[['Модуль упругости при растяжении, ГПа']], 
                                                                   test_size = 0.3, 
                                                                   random_state = 42, 
                                                                   shuffle = True
                                                                   )

# Посмотрим размерность тренировочной и тестовой выборок
print('Размер тренировочного датасета: {}\nРазмер тестового датасета:{}'.format(X_train_elastic.shape, X_test_elastic.shape))

X_train_elastic

y_train_elastic

"""Рассмотрим работу различных моделей на стандартных параметрах."""

# Создадим функцию, которая обрабатывает различные модели, переданные в качестве аргумента, и выдает для них метрики эффективности
# Метрики вычисляются как для тестовых данных, так и для тренировочных данных - тренировочные метрики находятся в ()
def Model_Comparision_Train_Test(AllModels, x_train, y_train, x_test, y_test):
    return_df = pd.DataFrame(columns=['Model', 'R2_score', 'MAE', 'MSE', 'RMSE', 'MAPE'])
    for myModel in AllModels:
        myModel.fit(x_train, y_train)

        # Предсказание и вычисление метрик для тренирвоочного набора данных 
        y_pred_train = myModel.predict(x_train)
        r2_score_train, mae_train, mse_train, rmse_train, mape_train = extract_metrics_from_predicted(y_train, y_pred_train)
        
        # Предсказание и вычисление метрик для тестового набора данных 
        y_pred_test = myModel.predict(x_test)
        r2_score_test, mae_test, mse_test, rmse_test, mape_test = extract_metrics_from_predicted(y_test, y_pred_test)
        
        # Создадим обобщающий датафрейм для всех рассчитанных метрик
        summary = pd.DataFrame([[type(myModel).__name__,
                                         ''.join([str(round(r2_score_test,4)), " (", str(round(r2_score_train,4)), ")"]),
                                         ''.join([str(round(mae_test,2)), " (", str(round(mae_train,2)), ")"]),
                                         ''.join([str(round(mse_test,2)), " (", str(round(mse_train,2)), ")"]),
                                         ''.join([str(round(rmse_test,2)), " (", str(round(rmse_train,2)), ")"]),
                                         ''.join([str(round(mape_test,2)), " (", str(round(mape_train,2)), ")"])]],
                                         columns=['Model', 'R2_score', 'MAE', 'MSE', 'RMSE', 'MAPE'])
        return_df = pd.concat([return_df, summary], axis=0)

    # Установим в качестве индекса столбец с названием модели
    return_df.set_index('Model', inplace=True)
    return(return_df)

# Создадим функцию для расчета метрик
def extract_metrics_from_predicted(y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    return (r2, mae, mse, rmse, mape)

# Рассчитаем метрики для следующих моделей:
KNR = KNeighborsRegressor()
SVReg = SVR()
LR = LinearRegression()
DTR = DecisionTreeRegressor()
Abr = AdaBoostRegressor(base_estimator=DecisionTreeRegressor())
Gbr = GradientBoostingRegressor()
XgbR = XGBRegressor()
RFR = RandomForestRegressor() #n_estimators = 20, max_depth = 10, random_state = 42)
SGDR = SGDRegressor()
LassoR = Lasso(alpha=0.1)

Model_Comparision_Train_Test([KNR, SVReg, LR, DTR, Abr, Gbr, XgbR, RFR, SGDR, LassoR], X_train_elastic, np.ravel(y_train_elastic), X_test_elastic, np.ravel(y_test_elastic))

"""Отрицательные значение коэффициента детерминации означают слабую обобщающую способность моделей.

Если R2 отрицательна, то модель работает хуже, чем простой подсчет среднего.

Лучшие показатели R2 и MAE на тестовой выборке у алгоритма регрессии «Lasso», на тренировочной выборке - у алгоритмов DecisionTreeRegressor, XGBRegressor и AdaBoostRegressor.

Обратим внимание на параметр MAPE - неотрицательный с плавающей запятой. Наилучшее значение равно 0.0. 
Но необходимо учесть, что неверные прогнозы могут привести к сколь угодно большим значениям MAPE, особенно если некоторые y_true значения очень близки к нулю.
"""

# Визуализируем прогнозные результаты y_pred для каждой модели
def Model_Comparision_Visualization(AllModels, x_train, y_train, x_test, y_test):
    for i, myModel in enumerate(AllModels, 1):
        myModel.fit(x_train, y_train)
        
        # Предсказание значений для тестового набора данных
        y_pred_test = myModel.predict(x_test)
        
        # Рисуем графики для значений у: тестового и прогнозного
        plt.figure(figsize = (50, 10))
        plt.subplot(n_rows, n_cols, i)      
        plt.plot(y_pred_test, label = "Прогноз", color = "darkmagenta")
        plt.plot(y_test, label = "Тест", color = "gold")
        plt.xlabel("Номер наблюдения", size=10)
        plt.ylabel("Модуль упругости при растяжении, ГПа", size=10)
        plt.title(type(myModel).__name__, size=15)
        plt.legend()
        plt.show()         
    
n_subplots = len([KNR, SVReg, LR, DTR, Abr, Gbr, XgbR, RFR, SGDR, LassoR])          # считаем количество графиков
n_cols = 4                                                                          # задаем количество столбцов
n_rows = (n_subplots + n_cols - 1) // n_cols                                        # вычисляем количество строк

# Выводим на экран графики прогнозных и тестов значений для каждой модели
Model_Comparision_Visualization([KNR, SVReg, LR, DTR, Abr, Gbr, XgbR, RFR, SGDR, LassoR], X_train_elastic, np.ravel(y_train_elastic), X_test_elastic, np.ravel(y_test_elastic))

"""Задачу выбора модели усложняет тот факт, что у многих типов моделей существуют разные вариации, особые параметры. Гиперпараметр модели – это численное значение, которое влияет на работу модели, но не подбирается в процессе обучения. Они задаются при определении модели и должны оставаться неизменными до схождения алгоритма обучения. 

Поиск по сетке – полный перебор всех комбинаций значений гиперпараметров для поиска оптимальных значений. Для его организации надо задать список гиперпараметров и их конкретных значений. Поиск по сетке имеет экспоненциальную сложность. Чем больше параметров и значений задать, тем лучше получится модель, но дольше поиск. Рекомендуется использовать кросс-валидацию. По умолчанию используется оценка модели, встроенная в сам объект модели через метод score, то есть точность (accuracy) для классификации и коэффициент детерминации (r-квадрат) для регрессии.
"""

# Проведем поиск гиперпараметров моделей с помощью поиска по сетке с перекрестной проверкой, количество блоков равно 10 (cv = 10)
# Воспользуемся методом GridSearchCV() 

# Создадим функцию для выбора лучшей модели
def Model_Selection(x_train, y_train, x_test, y_test):
    best_score = -10.0
    best_regressor = 0
    best_model = ''
    
    # Создаем список всех моделей регрессоров
    regressors = ['kneighborsregressor', 'svr', 'linearregression', 'decisiontreeregressor', 'adaboostregressor', 
                  'gradientboostingregressor', 'xgbregressor', 'randomforestregressor', 'sgdregressor', 'lasso']

    # Создаем словарь с набором всех используемых компонентов пайплайнов
    all_models = {'kneighborsregressor': KNeighborsRegressor(),
                  'svr': SVR(),
                  'linearregression': LinearRegression(),
                  'decisiontreeregressor': DecisionTreeRegressor(),
                  'adaboostregressor': AdaBoostRegressor(base_estimator = DecisionTreeRegressor()),
                  'gradientboostingregressor': GradientBoostingRegressor(),
                  'xgbregressor': XGBRegressor(),
                  'randomforestregressor': RandomForestRegressor(),
                  'sgdregressor': SGDRegressor(),
                  'lasso': Lasso()
                 }

    # Создаем словарь с наборами гиперпараметров всех моделей
    all_params = {'kneighborsregressor':       {'kneighborsregressor__n_neighbors': [i for i in range(1, 201, 2)],
                                                'kneighborsregressor__weights':     ['uniform', 'distance'],
                                                'kneighborsregressor__algorithm':   ['auto', 'ball_tree', 'kd_tree', 'brute']
                                               },
                  'svr':                       {'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
                                                'svr__C':      [0.01, 0.1, 1],
                                                'svr__gamma':  [0.01, 0.1, 1]
                                               },
                  'linearregression':          {'linearregression__fit_intercept': [True, False]},
                  'decisiontreeregressor':     {'decisiontreeregressor__max_depth':         [3, 5, 7, 9, 11, 13, 15],
                                                'decisiontreeregressor__min_samples_leaf':  [1, 2, 5, 10, 20, 50, 100, 150, 200],
                                                'decisiontreeregressor__min_samples_split': [200, 250, 300],
                                                'decisiontreeregressor__max_features':      ['auto', 'sqrt', 'log2']   
                                               },       
                  'adaboostregressor':         {'adaboostregressor__base_estimator__max_depth':        [i for i in range(2, 11, 2)],
                                                'adaboostregressor__base_estimator__min_samples_leaf': [5,  10],
                                                'adaboostregressor__n_estimators':                     [10, 50, 100, 250, 1000],
                                                'adaboostregressor__learning_rate':                    [0.01, 0.05, 0.1, 0.5]
                                               },
                  'gradientboostingregressor': {'gradientboostingregressor__learning_rate': [0.01,  0.02, 0.03, 0.04],
                                                'gradientboostingregressor__subsample'    : [0.9, 0.5, 0.2, 0.1],
                                                'gradientboostingregressor__n_estimators' : [100, 500, 1000, 1500],
                                                'gradientboostingregressor__max_depth'    : [4, 6, 8, 10]
                                               },
                  'xgbregressor':              {'xgbregressor__learning_rate':    [0.05, 0.10, 0.15],
                                                'xgbregressor__max_depth':        [ 3, 4, 5, 6, 8],
                                                'xgbregressor__min_child_weight': [ 1, 3, 5, 7],
                                                'xgbregressor__gamma':            [ 0.0, 0.1, 0.2],
                                                'xgbregressor__colsample_bytree': [ 0.3, 0.4]
                                               },         
                  'randomforestregressor':     {'randomforestregressor__n_estimators':     [30, 100, 200, 300, 500],
                                                'randomforestregressor__max_depth':        [1, 2, 3, 4, 5, 6, 7, 8],
                                                'randomforestregressor__min_samples_leaf': [1, 2],
                                                'randomforestregressor__max_features':     ['auto', 'sqrt', 'log2']
                                               }, 
                  'sgdregressor':              {'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],
                                                'sgdregressor__alpha':   [0.0001, 0.001, 0.01, 0.1]
                                               },
                  'lasso':                     {'lasso__alpha': [0.01, 0.02, 0.1, 0.2, 0.03, 0.3, 0.05, 0.5, 0.07, 0.7, 1]}
                 }

    # Запускаем основной цикл, на каждой итерации которого генерируем очередное сочетание всех этапов пайплайна
    for i, regressor in enumerate(regressors, 1):
        
        # Создаем pipeline для выбора наилучшей модели
        pipe = make_pipeline(all_models[regressor])
    
        # Создаем словарь гиперпараметров для пайплайна
        current_params = {}
        current_params.update(all_params[regressor])
        #print(current_params)
    
        # Выполняем обучение и оценку качества с помощью кросс-валидации, 
        model = GridSearchCV(estimator = pipe, param_grid = current_params,
                             scoring = 'r2', cv = 10, n_jobs = -1)
        model.fit(x_train, y_train)
       
        print('{} Лучшее значение R2 на тренировочной выборке: {:.4f}'.format(all_models[regressor], model.score(x_train, y_train)))
        print('{} Лучшее значение R2 на тестовой выборке {:.4f}'.format(all_models[regressor], model.score(x_test, y_test)))
        print('{} Лучшее значение R2 на перекрестной проверке: {:.4f}'.format(all_models[regressor], model.best_score_))
        print('{} Лучшие параметры модели: {}'.format(all_models[regressor], model.best_params_))
        print('\n______________________________________________________________________________________________________\n')
        if model.score(x_test, y_test) > best_score:
                best_score = model.score(x_test, y_test)
                best_model = model.best_estimator_
                best_regressor = regressor
    print('Регрессор с лучшим значением R2 = {:.4f} на тестовой выборке: {}\n'.format(best_score, all_models[best_regressor]))
    print('Лучший алгоритм:\n{}\n'.format(best_model))

Model_Selection(X_train_elastic, np.ravel(y_train_elastic), X_test_elastic, np.ravel(y_test_elastic))

# Рассчитаем метрики для моделей с их лучшими параметрами:
KNR_best = KNeighborsRegressor(algorithm='auto', n_neighbors=199, weights='uniform')
SVReg_best = SVR(C=0.01, gamma=1, kernel='rbf')
LR_best = LinearRegression(fit_intercept=True)
DTR_best = DecisionTreeRegressor(max_depth=9, max_features='sqrt', min_samples_leaf=200, min_samples_split=250)
Abr_best = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=2, min_samples_leaf=10), learning_rate=0.01, n_estimators=10)
Gbr_best = GradientBoostingRegressor(learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.5)
XgbR_best = XGBRegressor(colsample_bytree=0.3, gamma=0.2, learning_rate=0.05, max_depth=3, min_child_weight=5)
RFR_best = RandomForestRegressor(max_depth=1, max_features='log2', min_samples_leaf=2, n_estimators=100)
SGDR_best = SGDRegressor(alpha=0.1, penalty='l1')
LassoR_best = Lasso(alpha=0.01)

Model_Comparision_Train_Test([KNR_best, SVReg_best, LR_best, DTR_best, Abr_best, Gbr_best, XgbR_best, RFR_best, SGDR_best, LassoR_best], X_train_elastic, np.ravel(y_train_elastic), X_test_elastic, np.ravel(y_test_elastic))

"""Когда R2 отрицательно. Это означает, что модель хуже, чем предсказание среднего значения.

Модели плохо обучаются на тренировочной выборке и, соответственно, плохо предсказывают значения для тестовой выборки.

Коэффициент MAE одинаков у всех моделей.
"""

# Визуализируем прогнозные результаты y_pred для каждой модели после настройки гиперпараметров
Model_Comparision_Visualization([KNR_best, SVReg_best, LR_best, DTR_best, Abr_best, Gbr_best, XgbR_best, RFR_best, SGDR_best, LassoR_best], X_train_elastic, np.ravel(y_train_elastic), X_test_elastic, np.ravel(y_test_elastic))

"""Визуализация показывает, что после подбора гиперпараметров модели стали хуже прогнозировать результат.

Все использованные модели плохо справились с поставленной задачей прогнозирования модуля упругости при растяжении. 

Получен неудовлетворительный результат.

### _05.2. Прогнозирование прочности при растяжении.
"""

# Разделим датасет на тренировочную и тестовую выборки. 
# При построении модели 30% данных оставим на тестирование модели, на остальных происходит обучение моделей. 
X_train_strong, X_test_strong, y_train_strong, y_test_strong = train_test_split(
                                                               df_norm.drop(['Прочность при растяжении, МПа', 'Модуль упругости при растяжении, ГПа'], axis=1), 
                                                               df_norm[['Прочность при растяжении, МПа']], 
                                                               test_size = 0.3, 
                                                               random_state = 42, 
                                                               shuffle = True
                                                               )

# Посмотрим размерность тренировочной и тестовой выборок
print('Размер тренировочного датасета: {}\nРазмер тестового датасета:{}'.format(X_train_strong.shape, X_test_strong.shape))

X_train_strong

y_train_strong

# Рассчитаем метрики для всех моделей
Model_Comparision_Train_Test([KNR, SVReg, LR, DTR, Abr, Gbr, XgbR, RFR, SGDR, LassoR], X_train_strong, np.ravel(y_train_strong), X_test_strong, np.ravel(y_test_strong))

"""Когда R2 отрицательно, это означает, что модель хуже, чем предсказание среднего значения.

Лучшие показатели R2 и MAE на тестовой выборке у алгоритма регрессии «Lasso», на тренировочной выборке - у алгоритмов DecisionTreeRegressor, XGBRegressor и AdaBoostRegressor.
"""

# Визуализируем прогнозные результаты y_pred для каждой модели
Model_Comparision_Visualization([KNR, SVReg, LR, DTR, Abr, Gbr, XgbR, RFR, SGDR, LassoR], X_train_strong, np.ravel(y_train_strong), X_test_strong, np.ravel(y_test_strong))

# Проведем поиск гиперпараметров моделей с помощью поиска по сетке с перекрестной проверкой, количество блоков равно 10 (cv = 10)
Model_Selection(X_train_strong, np.ravel(y_train_strong), X_test_strong, np.ravel(y_test_strong))

# Рассчитаем метрики для моделей с их лучшими параметрами:
KNR_best = KNeighborsRegressor(algorithm='auto', n_neighbors=159, weights='distance')
SVReg_best = SVR(C=0.01, gamma=0.1, kernel='poly')
LR_best = LinearRegression(fit_intercept=True)
DTR_best = DecisionTreeRegressor(max_depth=15, max_features='sqrt', min_samples_leaf=100, min_samples_split=250)
Abr_best = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=2, min_samples_leaf=10), learning_rate=0.01, n_estimators=50)
Gbr_best = GradientBoostingRegressor(learning_rate=0.01, max_depth=8, n_estimators=100, subsample=0.1)
XgbR_best = XGBRegressor(colsample_bytree=0.3, gamma=0.2, learning_rate=0.05, max_depth=3, min_child_weight=5)
RFR_best = RandomForestRegressor(max_depth=1, max_features='log2', min_samples_leaf=2, n_estimators=30)
SGDR_best = SGDRegressor(alpha=0.1, penalty='l1')
LassoR_best = Lasso(alpha=0.01)

Model_Comparision_Train_Test([KNR_best, SVReg_best, LR_best, DTR_best, Abr_best, Gbr_best, XgbR_best, RFR_best, SGDR_best, LassoR_best], X_train_strong, np.ravel(y_train_strong), X_test_strong, np.ravel(y_test_strong))

"""Если R2 отрицательна, то модель работает хуже, чем простой подсчет среднего.

Отрицательные значение коэффициента детерминации означают слабую обобщающую способность моделей.

Коэффициент MAE одинаков у всех моделей.
"""

# Визуализируем прогнозные результаты y_pred для каждой модели после настройки гиперпараметров
Model_Comparision_Visualization([KNR_best, SVReg_best, LR_best, DTR_best, Abr_best, Gbr_best, XgbR_best, RFR_best, SGDR_best, LassoR_best], X_train_strong, np.ravel(y_train_strong), X_test_strong, np.ravel(y_test_strong))

"""Визуализация показывает, что после подбора гиперпараметров модели стали хуже прогнозировать результат.

Обратим внимание на параметр MAPE. Наилучшее значение данного параметра равно 0.0. Но необходимо учесть, что неверные прогнозы могут привести к сколь угодно большим значениям MAPE, особенно если некоторые y_true значения очень близки к нулю.

#### Вывод: 

Все использованные модели плохо справились с поставленной задачей прогнозирования прочности при растяжении.

Полученный результат не решает поставленную задачу.

С учетом полученных неудовлетворительных результатов в качестве прогноза для модуля упругости при растяжении и прочности при растяжении можно использовать среднее значение признака

## _06. Нейронная сеть для рекомендации соотношения "матрица - наполнитель".
"""

# Разделим датасет на тренировочную и тестовую выборки. 
# При построении модели 30% данных оставим на тестирование модели, на остальных происходит обучение моделей. 
X_train_matrix, X_test_matrix, y_train_matrix, y_test_matrix = train_test_split(
                                                               df_norm.drop(['Соотношение матрица-наполнитель'], axis=1), 
                                                               df_norm[['Соотношение матрица-наполнитель']], 
                                                               test_size = 0.3, 
                                                               random_state = 42, 
                                                               shuffle = True
                                                               )

# Посмотрим размерность тренировочной и тестовой выборок
print('Размер тренировочного датасета: {}\nРазмер тестового датасета:{}'.format(X_train_matrix.shape, X_test_matrix.shape))

# Создадим функцию для генерации слоев нейронной сети
def create_NN_model(layers, activation, drop, opt): 
        model= Sequential()
        for i, neurons in enumerate(layers):
            if i==0:
                model.add(Dense(neurons, input_dim=X_train_matrix.shape[1], activation = activation))  # входной слой
            else:
                model.add(Dense(neurons, activation))                                                  # добавляем полносвязный слой
                model.add(Dropout(drop))                                                               # исключаем переобучения
            model.add(Dense(1))                                                                        # выходной слой
        
            # Компиляция модели: определяем метрики и алгоритм оптимизации
            model.compile(loss = 'mse', optimizer = opt, metrics = ['mae'])
           
            return model

# Построим нейронную сеть с помощью поиска по сетке с перекрестной проверкой, количество блоков равно 5 (cv = 5)
# Воспользуемся методом GridSearchCV() 

reg = KerasRegressor(model = create_NN_model, layers = [128], activation = 'relu', drop = 0.1, opt = 'Adam', verbose = 2)

# Зададим параметры для модели
param_grid = {'activation': ['relu', 'softmax', 'sigmoid'],
                  'layers': [[128, 64, 16], [128, 128, 64, 32], [128, 128, 64, 16]],                 
                     'opt': ['Adam', 'SGD'],   
                    'drop': [0.0, 0.1, 0.2],   
              'batch_size': [10, 20, 40],     
                  'epochs': [10, 50, 100]
             }            
                                                
# Произведем поиск лучших параметров
grid = GridSearchCV(estimator = reg, 
                    param_grid = param_grid,
                    cv = 5,
                    verbose = 0, 
                    n_jobs = -1)

grid_result = grid.fit(X_train_matrix, np.ravel(y_train_matrix))

print('Лучший коэффициент R2: {:.4f} при использовании модели с параметрами {} \n'.format(grid_result.best_score_, grid_result.best_params_))

# Создадим модель с полученными значениями
best_model = Sequential()
best_model.add(Dense(128, input_dim = X_train_matrix.shape[1], activation = 'softmax'))      # входной слой
best_model.add(Dense(128, activation = 'softmax'))                                           # добавляем полносвязный слой
best_model.add(Dropout(0.0))                                                                 # исключаем переобучения
best_model.add(Dense(64, activation = 'softmax'))                                            # добавляем полносвязный слой
best_model.add(Dropout(0.0))                                                                 # исключаем переобучения
best_model.add(Dense(32, activation = 'softmax'))                                            # добавляем полносвязный слой
best_model.add(Dropout(0.0))                                                                 # исключаем переобучения
best_model.add(Dense(1))                                                                     # выходной слой
 
# Компиляция модели: определяем метрики и алгоритм оптимизации    
best_model.compile(loss = 'mse', 
                   optimizer = 'SGD', 
                   metrics = ['mae'])

# Обучение модели
best_history = best_model.fit(X_train_matrix, np.ravel(y_train_matrix), 
                              epochs=10,
                              batch_size=10,
                              verbose=1, 
                              validation_split=0.2)

# Оценка получившейся модели
best_model.evaluate(X_test_matrix, np.ravel(y_test_matrix), verbose = 1)

# Структура нейронной сети   
best_model.summary()

# Напишем функцию для построения графика потерь на тренировочной и тестовой выборок
def model_loss_plot(model_history):
    plt.figure(figsize = (25,15))
    plt.plot(model_history.history['loss'], label = 'Ошибка на обучающей выборке', color = "gold")
    plt.plot(model_history.history['val_loss'], label = 'Ошибка на валидацинной (тестовой) выборке', color = "darkmagenta")
    plt.title('График потерь модели', size=25, y=1.02)
    plt.ylabel('Значение ошибки MAE', size=20)
    plt.xlabel('Эпоха', size=20)
    plt.legend(prop={'size': 18})
    plt.show()

# Построим график потерь на тренировочной и тестовой выборках
model_loss_plot(best_history)

# Создадим функцию для визуализации прогнозных результатов y_pred для модели
def Comparision_Visualization_NN(y_test, y_pred):
    plt.figure(figsize=(15,7))
    plt.title(f'Тест и прогноз, Соотношение матрица-наполнитель', size=15)
    plt.plot(np.ravel(y_test), label = 'Тест', color = "gold")
    plt.plot(y_pred, label = 'Прогноз', color = "darkmagenta")
    plt.xlabel("Номер наблюдения", size=10)
    plt.ylabel("Соотношение матрица-наполнитель", size=10)
    plt.legend(loc='best')
    plt.show()

Comparision_Visualization_NN(y_test_matrix, best_model.predict(X_test_matrix))

"""#### Вывод: 

Результат прогноза нейронной сети неудовлетворительный. Значение функции потерь - среднего квадрата ошибки (MSE) - составило 0.0336, а средней абсолютной ошибки (MAE) - 0.1491.

Полученная модель нейронной сети плохо справились с поставленной задачей прогнозирования соотношения "матрица-наполнитель".
"""

# Сохраняем модель для дальнейшего испльзования в приложении
best_model.save('model_matrix', 'C:/Users/Asus/Documents/УЧЕБА_Data Science/0. ВКР')

# Сохраняем модель для дальнейшего испльзования в приложении
dump(best_model, 'best_model_matrix')

"""## _07. Разработка приложения для прогнозирования соотношения "матрица-наполнитель"

Создадим два варианта приложения:

* веб-приложение
* консольное

#### _07.1. Веб-приложение Flask.

Для удобства использования создан отдельный файл Application.ipynb
"""

# Инициализируем приложение Flask
app = Flask(__name__)

# Загружаем модель и масштабаторы
nn_model = keras.models.load_model('C:/Users/Asus/Documents/УЧЕБА_Data Science/0. ВКР/Application//model_matrix/')
scaler_x = load('C:/Users/Asus/Documents/УЧЕБА_Data Science/0. ВКР/Application//minmax_scl_x.pkl')
scaler_y = load('C:/Users/Asus/Documents/УЧЕБА_Data Science/0. ВКР/Application//minmax_scl_y.pkl')

# Определяем маршрут приложения для страницы веб-приложения по умолчанию
@app.route('/')
def home():
    return render_template('main.html')

# Создаем новый маршрут приложения, который считывает ввод из формы «main.html» 
# и при нажатии кнопки "Рассчитать" выводит результат
@app.route('/predict', methods = ['POST'])
def predict():
    int_features = [float(x) for x in request.form.values()]
    X = scaler_x.transform(np.array(int_features).reshape(1,-1))
    prediction = nn_model.predict(X)
    output = scaler_y.inverse_transform(prediction) 
    return render_template('main.html', 
                           prediction_text = 'Прогнозное значение соотношения "матрица - наполнитель": {}'.format(output[0][0]))

# Запуск сервера Flask
if __name__ == "__main__":
    app.run(debug=True, use_reloader=False)

"""Приложение выдает значение 2.914088010787964

Соотношение "матрица-наполнитель" для аналогичных показателей (позиция 3 датафрейма df_clean) составляет 2.767918
"""

df_clean.iloc[3]

"""#### _07.2. Консольное приложение."""

# Создадим функцию для ввода данных
def input_variable():
    x1 = float(input('Плотность, кг/м3: '))
    x2 = float(input('Модуль упругости, ГПа: '))
    x3 = float(input('Количество отвердителя, м.%: '))
    x4 = float(input('Содержание эпоксидных групп,%_2: '))
    x5 = float(input('Температура вспышки, С_2: '))
    x6 = float(input('Поверхностная плотность, г/м2: '))
    x7 = float(input('Модуль упругости при растяжении, ГПа: '))
    x8 = float(input('Прочность при растяжении, МПа: '))
    x9 = float(input('Потребление смолы, г/м2: '))
    x10 = float(input('Угол нашивки: '))
    x11 = float(input('Шаг нашивки: '))
    x12 = float(input('Плотность нашивки: '))
    return x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12

# Создадим функцию для вызова приложения
def app_model():
    # Загружаем модель и масштабаторы
    nn_model = load_model('C:/Users/Asus/Documents/УЧЕБА_Data Science/0. ВКР/Application//model_matrix/')
    scaler_x = load('C:/Users/Asus/Documents/УЧЕБА_Data Science/0. ВКР/Application//minmax_scl_x.pkl')
    scaler_y = load('C:/Users/Asus/Documents/УЧЕБА_Data Science/0. ВКР/Application//minmax_scl_y.pkl')
   
    print('Приложение прогнозирует соотношение "матрица-наполнитель"')
    for i in range(110):
        try:
            print('Введите "1" для прогноза, "2" для выхода')
            check = input()
      
            if check == '1':
                print('Введите данные для прогноза')
                X = input_variable()
                X =  scaler_x.transform(np.array(X).reshape(1,-1))
                prediction = nn_model.predict(X)
                output = scaler_y.inverse_transform(prediction) 
                print('Прогнозное значение соотношения "матрица-наполнитель": ')
                print(output[0][0])

            elif check == '2':
                break
            else:
                print('Повторите выбор')
                
        except Exception as e:
            print(e)
            print('Введены некорректные данные. Пожалуйста, повторите операцию')
            
# Запускаем приложение
app_model()

"""Приложение выдает значение 2.914088

Соотношение "матрица-наполнитель" для аналогичных показателей (позиция 3 датафрейма df_clean) составляет 2.767918
"""

df_clean.iloc[3]

"""## _08. Выводы:

В ходе работы с использованием разработанных алгоритмов была проведена обработка экспериментальных данных модуля упругости при растяжении, прочности при растяжении и соотношения «матрица-наполнитель» с использованием языка программирования python.
    
   Как показал анализ исходных данных, корреляционная зависимость между характеристиками композитов крайне слабая и стремится к нулю. Этот факт непосредственно повлиял на результат работы регрессионных моделей. Все использованные модели показали низкую прогнозирующую способность. Лучшим алгоритмом для прогноза модуля упругости при растяжении выбран AdaBoostRegressor, для прогнозирования прочности при растяжении – XGBRegressor.
    
   Созданная для рекомендации соотношения «матрица-наполнитель» нейронная сеть также плохо справилась с поставленной задачей прогноза. Такие низкие показатели работы алгоритмов машинного обучения говорят о том, что прогнозирование свойств композиционных материалов – достаточно сложный процесс, требующий  как знаний в области композиционных материалов, так и опыта в построении и использовании алгоритмов машинного обучения.
    
   Полученный неудовлетворительный результат может также свидетельствовать о недостатках и ошибках в наборе исходных данных, недостаточно глубокой и детальной обработке данных, неточностях в выборе алгоритмов машинного обучения и их параметров. 
    
   Таким образом, для успешного решения задачи, поставленной в выпускной квалификационной работе, необходимы более глубокие знания в области материаловедения и технологии конструкционных материалов, математического анализа и статистики, а также в области решения задач машинного обучения и обработки данных. Более детальное изучение данных вопросов и консультация квалифицированных специалистов из указанных областей определенно положительно повлияют на уточнение подходов и оптимизацию алгоритмов для решения задачи прогнозирования конечных свойств композиционных материалов.
"""